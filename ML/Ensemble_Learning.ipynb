{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXWTBYZoaO1yXzypnovGuJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptabhishek785/ML-DL-General_Material/blob/main/ML/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias vs Variance Tradeoff, Overfitting vs Underfitting\n",
        "\n",
        "Bias: Inability of the ML model to truely capture the relationship of the training data\n",
        "Variance: Difference of fits on different datasets e.g. training vs testing     \n",
        "\n",
        "\n",
        "Underfitting models have high bias and thus low variance\n",
        "Overfitting models have low bias and thus high variance\n",
        "\n",
        "\n",
        "- underfitting algos i.e. HB-LV: Logistic Regression, Linear Regression\n",
        "- overfitting algos i.e. LB-HV: DT(max_depth = None), SVM, KNN\n",
        "\n",
        "\n",
        "Most likely there is always a tradeoff between bias and variance. Ideally we want **low variance and low bias**\n",
        "\n",
        "Ways to achieve low bias and low variance\n",
        "1. Regularization\n",
        "2. Bagging\n",
        "3. Boosting"
      ],
      "metadata": {
        "id": "a-pIQf4ZC_v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Learning**:\n",
        "### Using Multiple different models for prediction and then aggregating to get final answer\n",
        "\n",
        "\n",
        "\n",
        "**Core Idea**: Wisdom of the crowd. Collectively with Diversity outperforms individual brilliance\n",
        "\n",
        "Multiple different models created using:\n",
        "- Different base models on same training data e.g. Decision Trees, Logistic Regression, SVM, etc\n",
        "- Same base models but different training data\n",
        "- Combination of the above two\n",
        "\n",
        "\n",
        "MAthematically,\n",
        "- if base models have accuracy >= .5\n",
        "- Ensemble Accuracy >= Accuracy of the worst model\n",
        "i.e. **Ensemble atleast outperforms the worst model and in best case even outperforms the best model**\n",
        "\n",
        "\n",
        "\n",
        "**Types of Ensemble:**\n",
        "1. Voting Ensemble (Regressor and Classifier)\n",
        "2. Bagging (Random Forest)\n",
        "3. Boosting (AdaBoost, Gradient Boosting, XgBoost)\n",
        "4. Stacking"
      ],
      "metadata": {
        "id": "MI6B37GF7-NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting Ensemble\n",
        "\n",
        "Training: Multiple different Base Models\n",
        "Prediction:    \n",
        "- For Regressor: Mean of individual model predictions\n",
        "- Classification: Majority vote\n",
        "\n",
        "\n",
        "Types of voting:     \n",
        "- **Hard Voting:** In hard voting, the predicted output class is a class with the highest majority of votes    \n",
        "- **Soft Voting:** In soft voting, the output class is the prediction based on the average of probability given to that class.\n",
        "- There is no one better than other. Try both to find the best one\n",
        "- Generally, soft voting provides a smoother decision boundary"
      ],
      "metadata": {
        "id": "H_l_XTSG9vHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging i.e. Bootstrapping(Random Resampling with Replacement) + Aggregating\n",
        "\n",
        "Training: Same Base Models(Any)\n",
        "Prediction: Bootstrapped training data i.e. resampling same data with replacement to create different samples\n",
        "- For Regressor: Mean of individual models prediction\n",
        "- Classification: Majority vote\n",
        "\n",
        "Note: Bagging is not that used, Boosting is preferrred over bagging since it generally gives better results\n",
        "\n",
        "\n",
        "Variations in Bagging (using trainging data):\n",
        "To implement change Hyperparameters in Bagging model\n",
        "1. Pasting: Resampling is done without replacement\n",
        "2. Random Subspacing: Instead of row sampling, column sampling is done with replacement\n",
        "3. Random Patches: Both column sampling and row sampling is done with replacement\n",
        "\n",
        "\n",
        "Out of Bags: Generally there are some rows that never end up in any of the bags, thus they are never used for training and testing. So OOB data can be used for testing. Keep Hyperparameter OOB = True\n",
        "\n",
        "bag.oob_score_\n",
        "\n",
        "\n",
        "Note:\n",
        "1. Bagging generally gives better results than pasting\n",
        "2. Good results generally come at 25% to 50% row sampling\n",
        "3. Random PAtches and subpatches should generally be used with high dimensionality data\n",
        "4. Hyperparamter tuning using GridSearchCV/ RandomSearchCV"
      ],
      "metadata": {
        "id": "K4GLlCgjASIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest i.e. Boosting using Decision Tree Models only\n",
        "Random forest gets its name from the fact that there is random sampling and multiple decision trees that make up a forests\n",
        "\n",
        "\n",
        "Features:\n",
        "- Gives one of the best results most of the times along with (Gradient Boosting, XgBoost thus almost used all the time)\n",
        "- DTs works on both classification as well as Regression and thus RF as well\n",
        "- Does not need much Hyperparameters tuning and thus works well with defaul parameters\n",
        "\n",
        "Difference between Random Forest and Bagging\n",
        "1. In Random Forest the base estimator should only be Decision Trees\n",
        "2. In Bagging when doing column sampling once the columns are sampled the same data is used throughout the node creation in that particular tree whereas in RFs columns are sampled again at every node creation thus bringing more variations in the training data\n",
        "3. Tree level sampling (Bagging) vs Node level Sampling (RF)\n",
        "\n",
        "Thus we have two different APIs and bagging with base estimator = None is not same as RF\n",
        "\n",
        "in a random forest, rows are same throughout the tree, while columns (features) are sampled without replacement to introduce diversity among the trees.\n",
        "\n",
        "\n",
        "\n",
        "Hyperparameters in RFs:\n",
        "- num of estimators = No of DTs\n",
        "- MAx_features = No of features at each node to be sampled\n",
        "- Bootsrap = with or without repalcement\n",
        "- max_samples = No of rows\n",
        "\n",
        "\n",
        "- Max_depth = Depth of the DT\n",
        "- criterion = Gini, entropy, etc\n",
        "- min_samples_split\n",
        "- min_samples_leaf\n",
        "\n",
        "- n_jobs\n",
        "- verbose\n",
        "- random_state\n",
        "- warm_job\n",
        "- class_weight"
      ],
      "metadata": {
        "id": "nFC8P69wd2ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of Bags (OOB):\n",
        "\n",
        "Bags are samples that are created and fed to train the data.\n",
        "Statistically, 37% of the samples end up not being used even once and are called Out of Bag (OOB) samples.\n",
        "\n",
        "We can use these OOB samples as validation set"
      ],
      "metadata": {
        "id": "mOhMd1OWmMlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance\n",
        "\n",
        "feature_importance_ is a Hyperparameter that is used for feature selection to select important features. Feature Importance Hyperparameter gives a score to each features based on its importance in determining the output i.e. features ranked based on max variations\n",
        "\n",
        "- In CampusX day 65: he teaches how the formula to calculate the feature_importance_score (Not Vimp to know)"
      ],
      "metadata": {
        "id": "BsF-oFw3qjF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Validation\n",
        "\n",
        "OFten times we have limited data fro training and testing and using plain train test split is not a good option. We instead use cross validation, where the dataset is divided into multiple parts called folds and 1 fold is used for testing and rest for training, switching the testing fold for all k times. Thus we are able to extract new data  from the same dataset\n",
        "\n",
        "-"
      ],
      "metadata": {
        "id": "0GNVWRuhQhQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XgBoost"
      ],
      "metadata": {
        "id": "CHMi4Ix8zeFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Powerful Algos: Works on all types of dataset\n",
        "\n",
        "- Random Fores\n",
        "- SVM\n",
        "- Gradient Boosting\n",
        "\n",
        "XgBoost likely outperforms all other algos. XgBoost is not an algorithm but a library built on Gradient Boosting. XgBoost optimizes Gradient Boosting algos interms of performance and speed using software engineering concpets like DSA, etc\n",
        "\n",
        "- Gradient Boosting is robust, flexible. XgBoost works over Gradient Boost and open source\n",
        "\n",
        "Positives\n",
        "- Flexible: Works on any platform , language, library integration, etc. It also works on all types of problems (classification, Timeseries, ranking, regression, anomaly, etc) and with deployment softwares\n",
        "\n",
        "- Speed: Uses Parallel processing, Data Structures, Cache, distributed computing, GPU, Out of core Computing and other software engineering concepts\n",
        "\n",
        "- Performance: has better performance becasue it has a better handling of missing values, sparse matrix, regularized learning objective. tree pruning, efficient split finding (weighted quantitle sketch + approximate tree learning), etc    \n",
        "\n",
        "LightGBM and CatBoost are similar algos"
      ],
      "metadata": {
        "id": "VGOV6ktF2BVs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CZKmDdmz5Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression: Supervised Regression problems\n",
        "\n",
        "Types:\n",
        "1. Simple Linear Regression - One Input (feature) and one target\n",
        "2. Multiple Linear Regression - Multiple Inputs and one output\n",
        "3. Polynomial Linear Regression - Non linear input data\n",
        "\n",
        "Aim: To create a line (slope and y-intercept)based on input data to predict output. Slope of the line also denotes the weightage to that feature, or how much change gets affected by a change in x.\n",
        "\n",
        "\n",
        "- There are two ways to calculate the values of m & b.\n",
        "1. Closed form solutions: using direct formula    \n",
        "e.g. Ordinary Least Squares (OLS). This is straightforward to calculate but becomes computationally complex when dimensionality increases. scikit learn implements OLS via LinearRegression class\n",
        "\n",
        "2. Non-closed form solutions. e.g. Gradient Descent. Based on optimization/ approximation techniques but easier to compute. SGD Regressor uses Gradient descent in scikit learn\n"
      ],
      "metadata": {
        "id": "kTKygmmB9lCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Regression: When we are working with more than 1 column\n",
        "\n",
        "Aim: To create a hyperplane (multiple slopes/ weights and y-intercept)based on input data to predict output such that the loss function is miniized.\n",
        "\n",
        "\n",
        "Note: The generalizaqtion is similar for Multivariate Regression. However the mathematics is complex and thus would recommend going through the formulaes\n"
      ],
      "metadata": {
        "id": "1odikX-kAxFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function (J) = Sum of the square of the errors. We find the values of m & b which minimizes the loss function\n",
        "\n",
        "Error = Actual - Predicted = yi - yi^"
      ],
      "metadata": {
        "id": "RXjxfch5VK8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: We will be using 2 base dataset for practicing the code\n",
        "# Boston datset for Regression\n",
        "# Iris datset for Classification\n",
        "\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data_iris = datasets.load_iris()\n",
        "data_california = datasets.fetch_california_housing()\n",
        "X = data_california.data\n",
        "y = data_california.target"
      ],
      "metadata": {
        "id": "yDmHAAlUC0xT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)"
      ],
      "metadata": {
        "id": "w1ps5t3oqx-6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBU6LJM5q2CX",
        "outputId": "de8b23e6-9916-414e-e232-87d170f21275"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.80991017, 1.25276895, 1.44720173, ..., 1.57213375, 1.43077765,\n",
              "       2.60233972])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr.intercept_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyMi9cver2hH",
        "outputId": "73bb5832-7b46-47b2-89ce-45030bd68043"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-36.022830906529414"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
        "\n",
        "r2_score(y_test, lr.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZwJIdSQsdQ1",
        "outputId": "a0066930-26c6-45cc-a6e1-c84e81441c4a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6013853272055167"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}